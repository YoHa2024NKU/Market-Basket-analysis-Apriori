{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market Basket Analysis Using Apriori /FP-Growth algorithm.\n",
    "Medhanie Yonatan Haile \n",
    "2120246059\n",
    "Software Engineering NKU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import fpgrowth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing the sales dataset transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the sales dataset transaction\n",
    "dataset = pd.read_csv(\"Sales1998_normalized.csv\",header=None, engine='python')\n",
    "# printing the shape of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically determine the number of columns\n",
    "num_columns = dataset.shape[1]\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Convert the transactions to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert the transactions to a list\n",
    "transactions = []\n",
    "for i in range(len(dataset)):\n",
    "     transactions.append([str(dataset.values[i, j]) for j in range(num_columns)])\n",
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows processed: {len(transactions)}\")\n",
    "print(dataset.isnull().sum())  # Check for missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the transaction dataset into a one-hot encoded DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the transaction dataset into a one-hot encoded DataFrame, where each column represents a unique item.\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert column names from float to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to integers if they are numeric, otherwise keep them as strings\n",
    "def convert_column_names(column):\n",
    "    try:\n",
    "        return int(float(column))  # Convert to integer if possible\n",
    "    except ValueError:\n",
    "        return str(column)  # Keep as string if conversion fails\n",
    "\n",
    "df.columns = [convert_column_names(col) for col in df.columns]\n",
    "# Verify the updated column names\n",
    "print(\"Updated Column Names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove any rows in the DataFrame dataset that contain missing values (NaN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verification step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame columns in to a list format\n",
    "print(\"Columns in df:\", df.columns)\n",
    "# Display the shape of the DataFrame rows and columns\n",
    "print(\"Shape of df:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if the column '177' exists in the DataFrame for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the column '177' exists in the DataFrame\n",
    "if 177 in df.columns:\n",
    "    # Filter transactions where item '177' is present\n",
    "    transactions_with_177 = df[df[177] == 1]\n",
    "    print(\"Transactions containing item 177:\")\n",
    "    print(transactions_with_177)\n",
    "else:\n",
    "    print(\"Item '177' is not found in the DataFrame columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analyze co-occurrence of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the co-occurrence matrix\n",
    "co_occurrence = df.T.dot(df)\n",
    "\n",
    "# Print the co-occurrence matrix\n",
    "print(\"Co-occurrence Matrix:\")\n",
    "print(co_occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out infrequent items\n",
    "item_counts = df.sum(axis=0)\n",
    "frequent_items = item_counts[item_counts >= 5].index\n",
    "df = df[frequent_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower the min_support threshold\n",
    "min_support = 0.0001  # Further reduced support threshold\n",
    "model = fpgrowth(df, min_support=min_support, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for larger itemsets\n",
    "print(\"Frequent Itemsets with More Than One Item:\")\n",
    "larger_itemsets = model[model['itemsets'].apply(lambda x: len(x) > 1)]\n",
    "print(larger_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This checks if there are any larger itemsets (itemsets with more than one item) in the frequent itemsets.\n",
    "If frequent itemsets and larger itemsets exist, the code generates association rules using the `association_rules` function.\n",
    "  - The rules are filtered based on the `min_confidence` threshold (set to `0.05` in this case).\n",
    "  - Confidence** measures the likelihood of the consequent being present in a transaction, given that the antecedent is present.\n",
    "   If rules are generated, they are sorted by the **lift** metric in descending order.\n",
    "  - Lift measures the strength of the association between the antecedent and the consequent. A lift value greater than 1 indicates a positive association.\n",
    "  - The sorted rules are printed, showing key metrics such as `antecedents`, `consequents`, `support`, `confidence`, and `lift`. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any frequent itemsets were found\n",
    "if model.empty:\n",
    "    print(\"No frequent itemsets found. Try lowering the min_support value.\")\n",
    "elif larger_itemsets.empty:\n",
    "    print(\"No larger itemsets found. Try lowering the min_support value.\")\n",
    "else:\n",
    "    # Lower the min_confidence threshold\n",
    "    min_confidence = 0.05  # Reduced confidence threshold\n",
    "    rules = association_rules(model, metric='confidence', min_threshold=min_confidence)\n",
    "\n",
    "    # Debug: Check the generated rules\n",
    "    print(\"Generated Rules:\")\n",
    "    print(rules)\n",
    "\n",
    "    # Check if any rules were generated\n",
    "    if rules.empty:\n",
    "        print(\"No rules generated. Try lowering the min_confidence value.\")\n",
    "    else:\n",
    "        # Sort rules by lift\n",
    "        rules = rules.sort_values(by='lift', ascending=False)\n",
    "\n",
    "        # Print the rules\n",
    "        print(\"Association Rules:\")\n",
    "        print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
